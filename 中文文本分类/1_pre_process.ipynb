{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './cnews/cnews.train.txt' #训练集\n",
    "test_file = './cnews/cnews.test.txt'  #测试集\n",
    "val_file = './cnews/cnews.val.txt'  #验证集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output file 分词后的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_train_file = './cnews/cnews.train.seg.txt'\n",
    "seg_test_file = './cnews/cnews.test.seg.txt'\n",
    "seg_val_file = './cnews/cnews.val.seg.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词表文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = './cnews/cnews.vocab.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类别文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_file = './cnews/cnews.category.txt'\n",
    "\n",
    "with open(train_file,'r',encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看训练文本的标签和内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育\n",
      "马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈\n"
     ]
    }
   ],
   "source": [
    "label, content = lines[0].strip('\\r\\n').split('\\t')\n",
    "print(label)\n",
    "print(content[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\86189\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.635 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "马晓旭/意外/受伤/让/国奥/警惕/ /无奈/大雨/格外/青睐/殷家/军/记者/傅亚雨/沈阳/报道/ /来到/沈阳/，/国奥队/依然/没有/摆脱/雨水/的/困扰/。/7/月/31/日/下午/6/点/，/国奥队/的/日常/训练/再度/受到/大雨/的/干扰/，/无奈/之下/队员/们/只/慢跑/了/25/分钟/就/草草收场/。/31/日/上午/10/点/，/国奥队/在/奥体中心/外场/训练/的/时候/，/天/就是/阴沉沉/的/，/气象预报/显示/当天/下午/沈阳/就/有/大雨/，/但/幸好/队伍/上午/的/训练/并/没有/受到/任何/干扰/。/下午/6/点/，/当/球队/抵达/训练场/时/，/大雨/已经/下/了/几个/小时/，/而且/丝毫/没有/停下来/的/意思/。/抱/着/试一试/的/态度/，/球队/开始/了/当天/下午/的/例行/训练/，/25/分钟/过去/了/，/天气/没有/任何/转好/的/迹象/，/为了/保护/球员/们/，/国奥队/决定/中止/当天/的/训练/，/全队/立即/返回/酒店/。/在/雨/中/训练/对/足球队/来说/并/不是/什么/稀罕/事/，/但/在/奥运会/即将/开始/之前/，/全队/变得/“/娇贵/”/了/。/在/沈阳/最后/一周/的/训练/，/国奥队/首先/要/保证/现有/的/球员/不再/出现意外/的/伤病/情况/以免/影响/正式/比赛/，/因此/这一/阶段/控制/训练/受伤/、/控制/感冒/等/疾病/的/出现/被/队伍/放在/了/相当/重要/的/位置/。/而/抵达/沈阳/之后/，/中/后卫/冯萧霆/就/一直/没有/训练/，/冯萧霆/是/7/月/27/日/在/长春/患上/了/感冒/，/因此/也/没有/参加/29/日/跟/塞尔维亚/的/热身赛/。/队伍/介绍/说/，/冯萧霆/并/没有/出现/发烧/症状/，/但/为了/安全/起/见/，/这/两天/还是/让/他/静养/休息/，/等/感冒/彻底/好/了/之后/再/恢复/训练/。/由于/有/了/冯萧霆/这个/例子/，/因此/国奥队/对雨中/训练/就/显得/特别/谨慎/，/主要/是/担心/球员/们/受凉/而/引发/感冒/，/造成/非战斗/减员/。/而/女足/队员/马晓旭/在/热身赛/中/受伤/导致/无缘/奥运/的/前科/，/也/让/在/沈阳/的/国奥队/现在/格外/警惕/，/“/训练/中/不断/嘱咐/队员/们/要/注意/动作/，/我们/可/不能/再出/这样/的/事情/了/。/”/一位/工作人员/表示/。/从/长春/到/沈阳/，/雨水/一路/伴随/着/国奥队/，/“/也/邪/了/，/我们/走/到/哪儿/雨/就/下/到/哪儿/，/在/长春/几次/训练/都/被/大雨/给/搅和/了/，/没想到/来/沈阳/又/碰到/这种/事情/。/”/一位/国奥/球员/也/对/雨水/的/“/青睐/”/有些/不解/。\n"
     ]
    }
   ],
   "source": [
    "word_iter = jieba.cut(content)\n",
    "print('/'.join(word_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成分词文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seg_file(input_file,output_seg_file):\n",
    "    \"\"\"\n",
    "    参数\n",
    "    input_file:输入的待分词文件名\n",
    "    output_seg_file:输出分好词的文件名\n",
    "    \"\"\"\n",
    "    #读入待分词的文件\n",
    "    with open(input_file,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    #输出分好词的文件\n",
    "    with open(output_seg_file,'w',encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            #去掉每行的回车换行符，以制表符将文件分割为两部分\n",
    "            #列表第一个是标签，剩余的是文本\n",
    "            #strip方法只能移除字符串开头结尾的指定内容\n",
    "            label , content = line.strip('\\r\\n').split('\\t')\n",
    "            word_iter = jieba.cut(content)\n",
    "            word_content = ''\n",
    "            for word in word_iter:\n",
    "                word = word.strip(' ')\n",
    "                if word != \"\":\n",
    "                    word_content +=word + ' '\n",
    "            #strip方法只能移除字符串开头结尾的指定内容\n",
    "            out_line = '%s\\t%s\\n'%(label,word_content.strip(' '))\n",
    "            f.write(out_line)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面三个函数均需被执行，以生成相应的分词文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seg_file(val_file,seg_val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seg_file(train_file,seg_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seg_file(test_file,seg_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词表文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab_file(input_seg_file,output_vocab_file):\n",
    "    \"\"\"\n",
    "    参数\n",
    "    input_file:输入的待生成词表的文件名\n",
    "    output_vocab_file:要输出的生成好的词表文件名\n",
    "    \"\"\"\n",
    "    #读入分好词的文件\n",
    "    with open(input_seg_file,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # dict {key , value} 词：词频\n",
    "    word_dict = {}\n",
    "    for line in lines:\n",
    "        #strip方法只能移除字符串开头结尾的指定内容\n",
    "        label , content = line.strip('\\r\\n').split('\\t')\n",
    "        #分好词的输入文件中，不同的词汇以空格隔开\n",
    "        for word in content.split(' '):\n",
    "            #setdefualt方法，字典的key不存在时则设置默认值\n",
    "            #word 不在字典里则word的频次为0\n",
    "            word_dict.setdefault(word,0)\n",
    "            word_dict[word] +=1\n",
    "            #等价写法\n",
    "            #word_dict[word] = word_dict.get(word,0) + 1\n",
    "    #按词频从大到小排序\n",
    "    sorted_word_dict = sorted(word_dict.items(),key = lambda d:d[1],reverse = True)            \n",
    "    \n",
    "    with open(output_vocab_file,'w',encoding='utf-8') as f:\n",
    "        f.write('<UNK>\\t99999999999\\n')\n",
    "        for item in sorted_word_dict:\n",
    "            f.write('%s\\t%s\\n'%(item[0],item[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成训练数据集的词表\n",
    "#### 注意：不需要生成测试数据集和验证数据集的词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_vocab_file(seg_train_file,vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成类别文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_category_file(input_file,output_category_file):\n",
    "    with open(input_file,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    category_dict = {}\n",
    "    for line in lines:\n",
    "        label , content = line.strip('\\r\\n').split('\\t')\n",
    "        category_dict[label] = category_dict.get(label,0) + 1\n",
    "    category_number = len(category_dict)\n",
    "    with open(output_category_file,'w',encoding='utf-8') as f:\n",
    "        for category in category_dict.keys():\n",
    "            out_line = \"%s\\n\"%category\n",
    "            print('{}\\t{}\\n'.format(category,category_dict[category]))\n",
    "            f.write(out_line)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意，只用生成训练数据集的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育\t5000\n",
      "\n",
      "娱乐\t5000\n",
      "\n",
      "家居\t5000\n",
      "\n",
      "房产\t5000\n",
      "\n",
      "教育\t5000\n",
      "\n",
      "时尚\t5000\n",
      "\n",
      "时政\t5000\n",
      "\n",
      "游戏\t5000\n",
      "\n",
      "科技\t5000\n",
      "\n",
      "财经\t5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_category_file(seg_train_file,category_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
